# kaggler-pub-GCI-titanic

このリポジトリは11/9に作成されました。

# コンペ概要

期限は11/18の午後５時まで

本家titanicコンペと比較してcolumnsは同じでデータ量も同じだが、内容が違う。これは、本家のtrain_dataとtest_dataをごちゃまぜにしたのか？
ー＞だとしたら、本家コンペのprivate_scoreが高いものを引っ張ってこれば解決しそう。

もしくは、train_dataとtest_dataに重複がないかを検討

# 試したいことリスト

年齢をビン化処理をする

他の学習機でのスコア向上

optunaを突かttあハイパーパラメーターチューニング（max_depth棟が大きくなってしまって過学習が起きてしまうのをどうするか要件等）

sex　*　Fareの列を作成するのがもしかしたら有効かもしれない

0_8.ipynbについてrfc以外の学習機に対してもハイパーパラメータ最適化を施したい。

# 変更内容

## 11/9土曜日

### 0.ipynbについて、

運営から配布されたデータ。

LBで0.765程度のスコアが出る。

## 0_8.ipynbについて、

現在LBで過去最高のスコア0.8が出る。

pro.ipynbに比べてlrでの予測値をアンサンブルしただけ。

### 1.ipynbについて、

前処理は名前に対しての敬称列を作成。

Familyの数を表す列を作成。

Ageをビン化

models = ['rfc', 'svc', "xgb", "gbc"] 

これらのモデルをoptunaによってハイパーパラメーター最適化

LBでは0.778程度しか出ない。

private_scoreでは0.85程度を記録

### 2.ipynbについて

前処理で各列の積についても考慮に入れてみた。

importanceを取得してみると、sex * Fare の要素が非常に大きな重要度を持っていることが判明した。

pycaretでtop3モデルgbc、ridge、xgboostのアンサンブルを作った。

なぜか、まったくスコアが出ない。private_scoreは0.83程度。

LBでは0.76程度しか出ない

pycaretを使うという策は断念。





